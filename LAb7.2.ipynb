{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af94102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2936806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ba381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041dd4b",
   "metadata": {},
   "source": [
    "<!-- sudo pip3 install tensorflow -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Download NLTK data\n",
    "nltk.download('movie_reviews')\n",
    "Load the movie reviews from NLTK's IMDb dataset\n",
    "reviews = [movie_reviews.words(fileid) for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f537ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Download NLTK data\n",
    "nltk.download('movie_reviews')\n",
    "Load the movie reviews from NLTK's IMDb dataset\n",
    "reviews = [movie_reviews.words(fileid) for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86244103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "filtered_reviews = []\n",
    "\n",
    "for review in reviews:\n",
    "    words = [word.lower() for word in review if word.lower() not in stop_words and word.lower() not in punctuation]\n",
    "    filtered_reviews.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2822e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "model = Word2Vec(filtered_reviews, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc204f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vector for a specific word\n",
    "word = \"movie\"\n",
    "word_vector = model.wv[word]\n",
    "print(f\"Word Vector for '{word}':\\n{word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc706ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for demonstration\n",
    "texts = [\"this movie is great\", \"awesome film\", \"worst movie ever\", \"I enjoyed it a lot\"]\n",
    "# Corresponding labels for each text\n",
    "labels = [\"positive\", \"positive\", \"negative\", \"positive\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73493a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts and train Word2Vec model\n",
    "tokenized_texts = [text.split() for text in texts]\n",
    "word2vec_model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e073315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts to sequences of word indices\n",
    "word_index = {word: index + 1 for index, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "X = [[word_index[word] for word in seq] for seq in tokenized_texts]\n",
    "X = pad_sequences(X, maxlen=max(len(seq) for seq in tokenized_texts), padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8278e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=100, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d585f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input text for testing\n",
    "input_text = \"I loved this movie!\"\n",
    "tokenized_input = input_text.split()\n",
    "\n",
    "# Convert the tokenized input text to word indices, handling out-of-vocabulary words\n",
    "input_indices = [word_index[word] if word in word_index else 0 for word in tokenized_input]\n",
    "\n",
    "# Pad the input sequence to match the maximum sequence length\n",
    "padded_input = pad_sequences([input_indices], maxlen=X.shape[1], padding='post')\n",
    "\n",
    "# Make predictions using the trained model\n",
    "prediction = model.predict(padded_input)\n",
    "predicted_label = \"positive\" if prediction[0][0] > 0.5 else \"negative\"\n",
    "\n",
    "print(f\"Input Text: {input_text}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
