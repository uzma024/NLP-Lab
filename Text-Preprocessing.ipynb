{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee20e32-4893-4dc4-889d-9376eddb7408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nText Processing Tasks:\\n- Stopword removal\\n- Word tokenization\\n- Sentence tokenization\\n- Cleaning => Tag removal\\n- Regular Expression matching => Find dates\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Processing Tasks:\n",
    "- Stopword removal\n",
    "- Word tokenization\n",
    "- Sentence tokenization\n",
    "- Cleaning => Tag removal\n",
    "- Regular Expression matching => Find dates\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a52576-8219-481a-8584-8b95e05e2e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e83978-1f1c-40c7-8031-fda60ed4a9e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urlopen\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize, RegexpTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f45641-d6cb-4626-b2a5-cf6cf12cd6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Apollo_program\"\n",
    "page = urlopen(url)\n",
    "page = page.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cebdc18a-5c6c-4c93-9bbb-bab4af97cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern_script = re.compile(r\"<script.*?>.*?</script>|<span.*?>.*?</span>|<style.*?>.*?</style>\",re.DOTALL)\n",
    "pattern_tag = re.compile(r\"<.*?>\", re.DOTALL)\n",
    "pattern_unicode = re.compile(\"&#.*?;\")\n",
    "page = re.sub(pattern_script, \"\", page) # Remove emedded scripts\n",
    "page = re.sub(pattern_tag, \"\", page) # Remove html tag\n",
    "page = re.sub(pattern_unicode, \"\", page) # Remove unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4978d303-b8d1-412a-bc57-ff6731822bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespaces = re.compile(\"\\n[\\n\\t]+[\\n\\t]\")\n",
    "page = re.sub(whitespaces, \" \", page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "19bbd6a1-fe9b-4251-941c-dad55678bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(page) # word tokenize\n",
    "sentences = sent_tokenize(page) # sentence tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8cd68b05-6ae1-4d74-b94d-28d9bb36cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words(\"english\"))\n",
    "filtered_senteces = [w for w in words if w.lower() not in stopwords] # Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aeb3979d-eaec-4f68-82b8-ba1461c38d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_upper(word): return word.upper()\n",
    "def to_lower(word): return word.lower()\n",
    "\n",
    "upper_words = [to_upper(w) for w in words]\n",
    "lower_words = [to_lower(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f6493927-8aec-485b-92e7-6053496189c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(w) for w in words] # Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e7d5f-2b3c-4ce9-9bfc-83764f81169c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
