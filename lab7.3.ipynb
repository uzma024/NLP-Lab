{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7565ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ec71a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e9a241a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "pos_reviews = [movie_reviews.words(fileid) for fileid in movie_reviews.fileids('pos')]\n",
    "neg_reviews = [movie_reviews.words(fileid) for fileid in movie_reviews.fileids('neg')]\n",
    "reviews = pos_reviews + neg_reviews\n",
    "labels = [1] * len(pos_reviews) + [0] * len(neg_reviews)\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "filtered_reviews = []\n",
    "\n",
    "for review in reviews:\n",
    "    words = [word.lower() for word in review if word.lower() not in stop_words and word.lower() not in punctuation]\n",
    "    filtered_reviews.append(\" \".join(words[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22c22f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text for which you want to generate embeddings\n",
    "input_text = \"I love natural language processing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eef434f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text\n",
    "tokenized_text = tokenizer.tokenize(input_text)\n",
    "tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "018d2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert token IDs to tensor\n",
    "input_tensor = torch.tensor([input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0206ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the BERT model output\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38ad8bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings:\n",
      "tensor([[[-0.0419,  0.0434, -0.2534,  ..., -0.2518,  0.2330,  0.6576],\n",
      "         [ 0.6340,  0.1373, -0.5598,  ..., -0.0658,  0.5974,  0.5542],\n",
      "         [ 1.1149,  1.2050,  0.3571,  ..., -0.0769,  0.4818,  0.4114],\n",
      "         ...,\n",
      "         [ 0.7493,  0.3590, -0.4548,  ..., -0.9851, -0.3001, -0.3983],\n",
      "         [ 0.5699,  0.1807, -0.3210,  ...,  0.3424, -0.4560, -0.3952],\n",
      "         [ 0.7647,  0.1535, -0.1123,  ...,  0.2889, -0.6728, -0.2865]]])\n",
      "Sentence Embedding:\n",
      "tensor([ 4.2820e-01,  3.5509e-01, -1.8181e-01, -4.7634e-02,  1.4228e-01,\n",
      "        -2.1608e-01, -6.9977e-03,  7.1022e-01,  9.0334e-03, -2.1341e-01,\n",
      "        -9.2669e-02, -8.8439e-02, -2.0178e-02,  6.1247e-02, -1.3540e-01,\n",
      "        -3.8201e-02,  1.1978e-01,  1.1908e-01, -1.6284e-01,  3.4451e-01,\n",
      "         3.7351e-01,  2.1084e-01, -2.2698e-01,  4.1619e-01,  6.0871e-01,\n",
      "        -2.1485e-01, -4.0662e-01,  1.5206e-01, -4.7932e-01, -3.1712e-01,\n",
      "        -5.8468e-02, -4.1601e-02, -2.3040e-01,  1.2369e-01, -1.9613e-01,\n",
      "        -3.3082e-02, -2.8607e-01, -1.4028e-01, -4.1479e-01,  1.0363e-01,\n",
      "        -2.1187e-01, -3.4186e-02, -2.8900e-01, -1.5692e-01, -1.2413e-02,\n",
      "        -4.5064e-02, -4.2889e-01,  1.6188e-01,  4.1027e-01,  1.7239e-01,\n",
      "        -3.4470e-01,  3.6419e-01, -6.7283e-02,  1.8065e-01,  1.0088e-01,\n",
      "         6.6506e-01, -1.5342e-01, -4.8022e-01, -4.6407e-02, -1.8790e-01,\n",
      "         1.4707e-01,  2.1240e-01,  4.5718e-02, -3.8051e-01,  4.9458e-01,\n",
      "        -5.9462e-02, -3.5252e-01, -3.0210e-01, -5.3367e-01, -1.1847e-02,\n",
      "        -3.7957e-01, -1.5059e-01,  3.3363e-01, -3.3575e-02, -1.6363e-01,\n",
      "         4.1289e-01, -2.0391e-01,  1.3401e-01, -8.7643e-02,  7.3488e-02,\n",
      "        -1.9432e-01,  6.7117e-01,  2.7853e-01,  3.5898e-01,  4.1271e-01,\n",
      "         1.7923e-01, -5.5737e-01, -8.0111e-02, -1.1607e-01, -3.0785e-01,\n",
      "        -1.5724e-01, -6.6723e-02, -2.1691e-01,  3.0996e-01,  6.2910e-01,\n",
      "         7.0531e-02, -2.3099e-01, -1.1467e-01, -3.6367e-01,  2.1000e-01,\n",
      "        -1.1751e-01, -5.3226e-01,  1.7010e-01,  5.1451e-01,  2.2559e-01,\n",
      "         1.6275e-01,  1.1413e-02,  1.0253e-01,  1.6672e-01, -1.6434e-01,\n",
      "         9.0424e-02, -2.8694e-01, -8.0962e-02, -5.0308e-01, -3.8515e-01,\n",
      "        -9.6233e-02,  1.5356e-01,  3.2735e-01,  1.1731e-01, -4.2862e-02,\n",
      "        -4.4681e-01, -1.4486e-01,  2.2688e-01,  8.8294e-01, -2.5520e-01,\n",
      "         1.6428e-03,  7.6396e-02, -3.1531e-02, -7.8485e-02, -7.9096e-01,\n",
      "         1.7177e-01,  7.5226e-01,  4.0400e-01, -1.5531e-01, -2.9425e-01,\n",
      "         2.9777e-01,  1.6838e-01, -3.2196e-01, -3.7397e-01,  5.8454e-02,\n",
      "        -1.1241e-01,  9.3118e-02, -1.0229e-02, -2.1962e-01,  2.6867e-01,\n",
      "         7.0148e-02, -9.5784e-02,  1.5682e-01,  4.6081e-01,  1.8685e-01,\n",
      "         5.1978e-01,  3.2095e-01, -3.8487e-01, -4.0196e-01, -1.5152e-01,\n",
      "         1.4343e-01, -2.8824e-01,  3.4694e-01,  6.1800e-01,  1.2478e-01,\n",
      "        -8.5969e-02, -3.0770e-01, -3.4478e-01,  2.1013e-01, -9.5629e-02,\n",
      "        -1.6453e-01,  5.2509e-02,  5.4096e-01, -2.2630e-01, -8.7335e-02,\n",
      "        -9.9036e-02,  1.6410e-02,  6.4803e-01,  4.9013e-03, -2.5915e-01,\n",
      "         8.6352e-02, -7.8136e-02,  4.6963e-01,  3.0506e-01, -5.9709e-02,\n",
      "        -2.3541e+00, -2.2309e-03,  2.4282e-01,  1.2660e-02, -1.2955e-02,\n",
      "        -4.1894e-01,  3.3595e-01, -6.8971e-01, -2.7733e-01, -4.5174e-02,\n",
      "        -6.5535e-02,  6.6658e-02, -2.9744e-01, -1.1347e-01,  5.6792e-01,\n",
      "        -2.3974e-01, -2.4740e-01,  1.4752e-01,  9.1354e-02,  3.5042e-01,\n",
      "        -1.6835e-01, -4.5449e-02,  2.8024e-02,  1.1835e-01,  6.7412e-02,\n",
      "         8.8389e-04,  1.6944e-01,  1.2431e-01, -1.0738e-01, -1.7048e-01,\n",
      "        -3.8144e-01,  6.3572e-01,  1.9789e-01,  4.7833e-02,  4.8339e-02,\n",
      "        -1.0904e-01, -7.9813e-02, -3.5326e-01, -2.7509e-01, -8.7845e-02,\n",
      "        -6.4344e-02, -1.9022e-01, -5.5597e-01,  4.9382e-01, -6.6025e-02,\n",
      "         1.8676e-01,  1.6580e-01,  5.1574e-01,  3.9168e-01,  2.5749e-01,\n",
      "         3.3492e-02, -3.2773e-01,  4.4262e-01, -1.3285e-01, -6.1331e-02,\n",
      "        -2.1402e-01, -5.7715e-02, -1.9692e-01,  2.3462e-02, -8.6875e-02,\n",
      "        -5.1172e-01, -4.6582e-02,  1.2360e-01,  1.7342e-01,  9.6683e-03,\n",
      "        -3.6069e-02, -8.7524e-02,  1.0189e-02,  1.0615e-01, -1.8491e-01,\n",
      "        -1.6162e-01, -2.0750e-01,  9.7982e-02, -2.7821e-01, -1.3999e-01,\n",
      "        -1.6377e-02, -1.9568e-01,  2.5895e-01,  1.2963e-02,  2.3928e-01,\n",
      "         2.7148e-01,  4.5824e-01,  4.1538e-01, -3.6392e-02, -4.2172e-01,\n",
      "        -4.5364e-01,  1.7060e-01,  5.1011e-02,  3.3727e-01,  1.0870e-02,\n",
      "        -1.5893e-01, -4.1214e-01,  1.4791e-01,  2.4127e-02, -3.5297e-01,\n",
      "        -8.5796e-01, -3.4744e-02, -2.3247e-01,  4.3115e-02, -4.4050e-01,\n",
      "        -1.6761e-01,  3.6690e-01, -1.9339e-01,  2.1038e-01, -1.5438e-01,\n",
      "        -1.7039e-01, -1.9387e-01, -9.4742e-02,  1.8549e-01, -3.5908e-01,\n",
      "        -2.1308e-01, -1.2925e-01, -2.9293e-01, -2.1829e-01,  2.7687e-01,\n",
      "         4.7577e-02,  4.0160e-01,  8.4299e-02,  3.2129e-02, -2.6575e-01,\n",
      "        -4.1861e-02, -2.8515e-01, -4.5407e-02, -2.4782e-01, -7.3326e-01,\n",
      "        -2.3658e-02,  6.7351e-02, -1.1168e-01, -2.4688e+00, -4.7460e-02,\n",
      "        -1.1457e-01, -3.8124e-01,  2.9394e-01,  3.4645e-01, -1.5861e-01,\n",
      "         4.3855e-02, -1.7570e-01,  2.2086e-02,  2.6082e-01, -2.1149e-01,\n",
      "         2.4911e-02,  1.9551e-01, -2.9570e-02, -3.5807e-02, -1.4737e-01,\n",
      "        -3.3835e-01, -5.5310e-01,  6.5973e-01,  1.2170e-01,  1.5558e-01,\n",
      "         2.4208e-01, -1.7721e-01,  3.9271e-01,  1.3339e-02, -4.7829e-01,\n",
      "         1.4188e-01, -3.5724e-01,  5.9680e-02, -1.1980e-01, -2.6867e-01,\n",
      "         4.2846e-02,  3.8141e-01, -2.3358e-02, -1.8002e-01, -2.3035e-02,\n",
      "         3.7125e-02,  1.8462e-01, -2.5128e-01, -9.5503e-02, -1.0102e-01,\n",
      "         1.3272e-01, -4.3996e-01,  3.7963e-01, -1.2528e-01, -8.7943e-02,\n",
      "        -3.3829e-01, -4.5646e-02,  2.1064e-01,  2.6399e-02,  1.2055e-01,\n",
      "         2.2279e-01,  1.0452e-01,  2.3324e-01, -2.2199e-01,  6.1771e-01,\n",
      "         2.9592e-01, -3.6222e-01, -1.7907e-01,  2.5789e-01, -5.3121e-01,\n",
      "        -3.9486e-02, -3.0666e-01,  1.2965e-01, -3.7632e-01, -7.6266e-01,\n",
      "        -4.8178e-01, -1.0200e-01,  1.5698e-01,  5.3189e-02, -1.5940e-01,\n",
      "        -3.7357e-01, -7.1791e-01, -2.6898e-01,  7.5114e-03, -2.8903e-01,\n",
      "        -1.2308e-01,  1.7917e-01,  1.9702e-02, -6.2427e-02, -4.6144e-01,\n",
      "        -1.3805e-01,  1.4533e-01, -1.1523e-02, -2.1639e-01, -4.0403e-01,\n",
      "        -2.7486e-01,  2.4699e-02, -2.5935e-01,  6.1295e-01,  6.0867e-02,\n",
      "         3.6326e-01,  8.6229e-02,  1.2577e-01,  4.3696e-03,  3.6120e-01,\n",
      "        -2.9969e-01,  2.9463e-01,  8.6798e-02, -1.1531e-01,  1.3261e-02,\n",
      "        -1.1976e-01,  8.9423e-02,  3.1092e-01,  2.3703e-01, -6.7689e-01,\n",
      "         1.6304e-02,  1.7588e-01,  1.9843e-01, -2.2053e-01, -1.7746e-01,\n",
      "         6.1761e-01, -8.3398e-02, -2.5781e-01, -9.9956e-02,  2.2066e-01,\n",
      "         4.0497e-01, -4.2639e-02, -1.2240e-01, -1.5134e-01,  5.9877e-01,\n",
      "        -4.9815e-01,  3.0998e-01,  7.4926e-03, -2.8998e-01, -1.7335e-02,\n",
      "        -4.8734e-02, -5.7347e-01, -2.8692e-02,  2.9831e-02, -1.4759e-01,\n",
      "         3.8048e-02, -8.6941e-02,  1.2681e-01, -2.1501e-01, -1.2322e-01,\n",
      "         3.1246e-01,  4.3026e-02,  1.0292e-01,  2.6458e-01,  1.3380e-01,\n",
      "         1.2219e-02, -4.2954e-02, -1.5197e-01,  3.9701e-01, -2.1764e-01,\n",
      "        -6.0181e-02,  2.0026e-02,  1.1804e-01, -7.4967e-02, -2.0521e-01,\n",
      "        -5.4659e-02, -2.8020e-01,  2.0660e-01, -9.8666e-02,  6.9147e-02,\n",
      "         1.4320e-03,  5.2310e-03, -3.3406e-01,  5.0286e-01,  9.9603e-02,\n",
      "         2.8542e-01,  2.2818e-02,  4.5300e-03, -2.8038e-02, -1.7501e-01,\n",
      "        -4.0958e-02, -3.4889e-01,  1.8528e-01,  2.0422e-01, -4.5184e-01,\n",
      "         9.3530e-02, -1.8863e-01,  1.1315e-01,  2.6261e-01,  2.3527e-02,\n",
      "        -2.3693e-01, -1.5502e-01,  5.2539e-01, -7.6829e-02, -5.4402e-01,\n",
      "        -9.3388e-02,  7.5002e-02,  4.5425e-01,  2.5634e-01,  2.6398e-01,\n",
      "        -7.6260e-02,  1.2521e-01, -2.1259e-01, -3.2705e-01,  3.1027e-01,\n",
      "        -2.8744e-01, -6.1029e-01, -4.7866e-01, -6.8461e-02,  5.2548e-01,\n",
      "        -1.3330e-01,  2.7638e-01,  1.3058e-01,  4.3448e-01, -6.8601e-02,\n",
      "        -1.6488e-01, -9.9660e-02,  1.3608e-01,  2.0739e-01, -4.5335e-02,\n",
      "         3.3094e-02,  2.9839e-02,  1.3366e-01, -2.7116e-01, -3.5440e-01,\n",
      "        -2.0144e-01, -7.9957e-01,  1.9162e-01,  2.8185e-01,  1.2691e-01,\n",
      "         6.5757e-02,  6.0717e-02, -8.2936e-02, -6.4690e-01,  1.9202e-01,\n",
      "         4.2753e-02,  1.3333e-01, -1.2222e-01, -4.0602e-01, -3.6099e-01,\n",
      "        -2.6391e-02, -3.8383e-01,  2.1913e-01,  5.5517e-02, -7.7325e-03,\n",
      "         7.9350e-02, -1.4007e-02,  4.1288e-01, -1.4981e-02,  1.1212e-01,\n",
      "         9.4972e-02,  5.4103e-01, -9.5330e-02,  1.6789e-01, -3.1832e-01,\n",
      "        -2.1384e-01,  4.1597e-01,  2.7199e-01, -1.3708e-01, -2.3690e-01,\n",
      "         3.3775e-01, -1.5008e-01, -8.0644e-02,  1.5726e-01,  1.1248e-01,\n",
      "        -2.4620e-01,  1.0856e-02, -4.9661e-02, -6.4514e-01, -4.8598e-01,\n",
      "         9.1724e-02, -2.0266e-01,  2.7145e-01, -9.3185e-02,  3.3295e-02,\n",
      "        -1.9879e-01,  4.0026e-01,  2.7908e-01,  3.8490e-01, -1.3393e-01,\n",
      "         3.8060e-01,  1.5906e-01,  2.3534e-01, -1.9811e-01, -3.5876e-01,\n",
      "        -1.1395e-01,  2.5366e-01, -3.2259e-01, -3.8944e-02,  2.0644e-01,\n",
      "        -1.7169e-01,  9.2732e-02, -4.8659e-01,  3.9037e-01,  2.2189e-01,\n",
      "         3.1247e-01, -7.7209e-01,  1.2698e-01, -2.0110e-01, -3.7799e-01,\n",
      "         8.7725e-02, -3.5545e-01,  1.2329e-01,  2.2395e-01,  3.2538e-01,\n",
      "        -9.6465e-02,  2.4201e-01, -1.4516e-01, -1.1262e-03,  1.2694e-01,\n",
      "         1.4682e-01, -2.1415e-01,  9.1052e-02, -1.1211e-01,  2.8831e-01,\n",
      "         2.2133e-01, -1.1603e-01, -3.5969e-02,  2.0681e-01,  2.5296e-01,\n",
      "         4.3866e-02,  6.0479e-02,  1.9960e-01, -4.7535e-01, -8.8240e-02,\n",
      "         6.0432e-01,  3.8903e-01, -5.1945e-01, -1.3320e-01,  4.5883e-01,\n",
      "        -3.4942e-01, -1.2779e-01,  2.2649e-01, -5.4824e-02, -2.1221e-01,\n",
      "         3.5225e-01,  5.8646e-02, -1.2222e-02,  6.8797e-01, -6.1193e-01,\n",
      "        -1.4212e-03,  2.8683e-01,  8.8171e-02,  5.3231e-03,  5.7673e-01,\n",
      "        -4.2047e-01,  5.0459e-01,  1.8867e-01, -2.1296e-01,  1.0720e-01,\n",
      "         2.3560e-01,  3.4720e-01,  5.9708e-02,  4.5199e-01,  1.0750e-01,\n",
      "        -1.1466e-01,  6.2520e-01,  1.2349e-02, -8.7798e-02,  2.8754e-01,\n",
      "         2.7946e-01,  3.8695e-01,  9.1768e-02,  3.5381e-01,  1.5855e-01,\n",
      "         3.6831e-01,  5.9188e-02, -9.6972e-02,  1.5275e-01,  3.6357e-01,\n",
      "         1.1734e-01,  3.2724e-01, -5.5884e-01,  1.1307e-01, -7.3628e-02,\n",
      "         5.4662e-02, -4.8614e-01,  2.3051e-01,  6.7453e-01,  2.0847e-01,\n",
      "        -2.0452e-01, -1.0184e-01, -6.2808e-02, -2.4330e-01,  6.5050e-02,\n",
      "        -1.2502e-01,  6.4280e-02, -1.2143e-01,  2.8344e-01, -1.9933e-01,\n",
      "         1.9344e-01, -2.7404e-01, -1.9196e-01,  1.7834e-01,  2.3959e-01,\n",
      "        -3.7784e-01, -2.8090e-01, -2.0535e-01, -4.1441e-01, -1.6252e-01,\n",
      "        -2.0445e-01,  1.7351e-02, -3.7209e-02, -5.6079e-02, -7.3909e-02,\n",
      "        -1.4651e-01, -5.2533e-01,  3.4434e-01, -2.5384e-01,  2.0751e-03,\n",
      "         3.4832e-01,  4.3227e-01,  1.1359e-01,  3.0365e-01, -7.8360e-02,\n",
      "        -1.9661e-01,  2.7957e-01, -1.6204e-01,  3.2518e-01, -1.7197e-04,\n",
      "         2.7837e-01, -2.1980e-01, -5.0771e-01,  1.8504e-02,  1.6979e-01,\n",
      "        -9.9187e-01,  1.8619e-01,  3.0523e-02,  3.2833e-01,  1.8105e-01,\n",
      "         1.3371e-01,  2.8108e-01, -1.1524e-01,  1.3179e-01, -2.8751e-01,\n",
      "         2.8221e-01,  4.9416e-01, -3.0961e-01,  2.0679e-01, -1.0007e-01,\n",
      "         1.1454e-01, -1.3731e-01, -2.5100e-01, -1.0955e-02,  2.6888e-01,\n",
      "        -1.4711e-01, -1.9953e-01,  4.5937e-01, -1.6839e-01, -1.5761e-01,\n",
      "        -3.0076e-02, -1.7926e-01, -3.1599e-01, -5.0269e-02,  9.5973e-02,\n",
      "         8.6548e-03, -1.4043e-01, -1.4883e+00,  1.6531e-01,  3.5280e-01,\n",
      "        -6.0885e-01, -5.2822e-02, -7.0767e-01,  1.8786e-01, -2.2025e-01,\n",
      "         1.4577e-01,  4.2305e-02,  2.1169e-01,  1.8478e-01,  3.1667e-01,\n",
      "        -2.6079e-01, -8.0917e-03,  1.3945e-01])\n"
     ]
    }
   ],
   "source": [
    "# Extract the embeddings from the BERT model outputs\n",
    "hidden_states = outputs[2]\n",
    "word_embeddings = hidden_states[-1]  # Last layer hidden states for each token\n",
    "\n",
    "# Average the token embeddings to get the sentence embedding\n",
    "sentence_embedding = torch.mean(word_embeddings, dim=1).squeeze()\n",
    "\n",
    "print(\"Word Embeddings:\")\n",
    "print(word_embeddings)\n",
    "print(\"Sentence Embedding:\")\n",
    "print(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3629263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54f7a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for text classification\n",
    "# texts = [\"I love natural language processing.\", \"This movie is great!\", \"I don't like this product.\", \"The weather today is nice.\"]\n",
    "# labels = [1, 1, 0, 1]  # Binary labels (1: positive, 0: negative)\n",
    "texts = filtered_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1472baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10709856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 classes for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2cc0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input texts\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8149e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "input_ids_train, input_ids_test, labels_train, labels_test = \\\n",
    "    train_test_split(tokenized_texts['input_ids'], labels, test_size=0.2, random_state=42)\n",
    "attention_masks_train, attention_masks_test = \\\n",
    "    train_test_split(tokenized_texts['attention_mask'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af2361b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "af7fc6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and testing data\n",
    "train_data = TensorDataset(input_ids_train, attention_masks_train, torch.tensor(labels_train))\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(input_ids_test, attention_masks_test, torch.tensor(labels_test))\n",
    "test_loader = DataLoader(test_data, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cf7d285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids_batch, attention_masks_batch, labels_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids_batch, attention_mask=attention_masks_batch, labels=labels_batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test data\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90098ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids_batch, attention_masks_batch, labels_batch = batch\n",
    "        outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
    "        true_labels.extend(labels_batch.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = torch.sum(torch.tensor(predictions) == torch.tensor(true_labels)).item() / len(true_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input text for testing\n",
    "input_text = \"I love this movie!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text\n",
    "tokenized_input = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_input)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "predicted_label = \"positive\" if prediction == 1 else \"negative\"\n",
    "\n",
    "print(f\"Input Text: {input_text}\")\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a8b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
